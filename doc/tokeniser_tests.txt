Tokeniser
---------

A tokenizer is created by calling the Tokenizer constructor with a list of token
definitions.  At least one token definition must be supplied:

  >>> from varas import *
  >>> empty = Tokenizer()
  Traceback (most recent call last):
  ...
  AssertionError: No token definitions supplied

Each token definition is a tuple of (regular expression string, token type).
The token type is a object that is used to indicate when a token of this type is
encountered.  It can be any object.  Useful possibilities are a constant integer
or string.

A tokenizer that splits its input into word and number tokens could be created
like this:

  >>> WORD_TOKEN = "WORD"
  >>> NUMBER_TOKEN = "NUM"
  >>> tokenizer = Tokenizer( ("\d+", NUMBER_TOKEN),
  ...                        ("\w+", WORD_TOKEN) )

Calling the tokenize() method returns a generator object, which in turn yields
instances of Token().  The tokenizer always includes a token of type
Token.END_TOKEN at the end to indicate the end of the input has been reached.

The tokenizer can be used like this:

  >>> list(tokenizer.tokenize("one 2 three"))
  [Token('WORD', 'one'), Token('NUM', '2'), Token('WORD', 'three'), Token(Token.END_TOKEN, '')]

Tokenizing an empty string, or one containing only whitespace yeilds only the
end token:

  >>> list(tokenizer.tokenize(""))
  [Token(Token.END_TOKEN, '')]

  >>> list(tokenizer.tokenize("  \n  "))
  [Token(Token.END_TOKEN, '')]

Attempting to parse invalid input will raise a ParseError:

  >>> list(tokenizer.tokenize("one 2!!! three"))
  Traceback (most recent call last):
  ...
  ParseError: Can't tokenize input at line 1 column 5

For token types which only match a fixed string -- for example an operators or
keyword in a computer language -- it is helpful to have the token type be the
string itself.  This can be done by passing None as the token type, and this
allows multiple token types to be set up with one token definition:

  >>> keywords = Tokenizer( ("one|two|three|four", None) )
  >>> list(keywords.tokenize("two four"))
  [Token('two', 'two'), Token('four', 'four'), Token(Token.END_TOKEN, '')]

If you use strings to represent other token types, it's important to make sure
they don't overlap with any keyword tokens produced like this.
